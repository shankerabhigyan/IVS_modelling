{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Two Step Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handler import preprocessData, fitSurface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"../data/raw/yearwise/spy_options_data_23.json\" #../data/raw/yearwise/spy_options_data_22.json\n",
    "csv_save_path = \"../data/processed/pca/predicted_iv23.csv\" # ../data/processed/predicted_iv22.csv\n",
    "preprocess = preprocessData(json_path)\n",
    "df = preprocess.fit()\n",
    "print(df.head())\n",
    "fit = fitSurface(df)\n",
    "predicted_iv = fit.fit()\n",
    "print(predicted_iv)\n",
    "predicted_iv.to_csv(csv_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import featureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"../data/processed/pca/predicted_iv16.csv\",\n",
    "    \"../data/processed/pca/predicted_iv17.csv\",\n",
    "    \"../data/processed/pca/predicted_iv18.csv\",\n",
    "    \"../data/processed/pca/predicted_iv19.csv\",\n",
    "    \"../data/processed/pca/predicted_iv20.csv\"\n",
    "]\n",
    "save_path = \"../data/processed/features_pca_iv16-20.csv\"\n",
    "\n",
    "fe = featureExtractor(paths, save_path)\n",
    "fe.transform()\n",
    "fe.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check to see if there are no repeated values or NaNs\n",
    "df = pd.read_csv(save_path)\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())\n",
    "print(df.shape)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae_master import VAE, IVSDataset, IVSFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm import CustomLSTMCell, CustomLSTMModel, ModelManager, DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_path = \"../data/processed/features_pca_iv16-20.csv\"\n",
    "# dataset = DatasetManager(pca_path)\n",
    "# features, targets = dataset.make_train_target_pairs()\n",
    "# print('Features shape:', features.shape)\n",
    "# print('Targets shape:', targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset into training and validation sets\n",
    "# split = int(0.8 * len(features))\n",
    "# train_features, val_features = features[:split], features[split:]\n",
    "# train_targets, val_targets = targets[:split], targets[split:]\n",
    "\n",
    "# no split\n",
    "# train_features, train_targets = features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Initializing model...')\n",
    "# model_path = './lstm1620_256.pth'\n",
    "# model = ModelManager(input_dim=9, hidden_dim=256, output_dim=3, model_path=model_path, learning_rate=0.01)\n",
    "# model.train(train_features, train_targets, epochs=160000)\n",
    "\n",
    "# #val_loader = DataLoader(TensorDataset(val_features, val_targets), batch_size=1, shuffle=False)\n",
    "# #model.validate(val_loader)\n",
    "\n",
    "# model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from models.lstm import CustomLSTMCell, CustomLSTMModel, ModelManager, DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_path = \"./ckpts/lstm1620_256.pth\"\n",
    "#lstm_model_path = './ckpts/test_bilstm256.pth'\n",
    "lstm_model = CustomLSTMModel(input_dim=9, hidden_dim=256, output_dim=3)\n",
    "lstm_model.load_model(model_path=lstm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"../data/processed/features_pca_iv16-20.csv\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(22,len(features)):\n",
    "    ma1 = torch.tensor(features.iloc[i-1][['feature1', 'feature2', 'feature3']].astype(float).values, dtype=torch.float32)\n",
    "    ma5 = torch.tensor(features.iloc[i-5:i][['feature1', 'feature2', 'feature3']].mean(axis=0).values, dtype=torch.float32)\n",
    "    ma22 = torch.tensor(features.iloc[i-22:i][['feature1', 'feature2', 'feature3']].mean(axis=0).values, dtype=torch.float32)\n",
    "    feature = torch.cat((ma1, ma5, ma22), dim=0).to(device)\n",
    "    out = lstm_model.predict(feature)\n",
    "    for obj in out:\n",
    "        features.at[i, \"F1\"] = obj[0].item()\n",
    "        features.at[i, \"F2\"] = obj[1].item()\n",
    "        features.at[i, \"F3\"] = obj[2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.dropna().reset_index(drop=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv_path_list = [\n",
    "    \"../data/processed/pca/predicted_iv16.csv\",\n",
    "    \"../data/processed/pca/predicted_iv17.csv\",\n",
    "    \"../data/processed/pca/predicted_iv18.csv\",\n",
    "    \"../data/processed/pca/predicted_iv19.csv\",\n",
    "    \"../data/processed/pca/predicted_iv20.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for path in df_iv_path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    merged_df = pd.concat([merged_df, df], axis=0)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "print(len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes using the date column so that we have the corresponding F1, F2, F3 values for each date\n",
    "df = pd.merge(merged_df, features, on='date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['F1', 'F2', 'F3']\n",
    "from models.dnn import IVDataset, IVSDNN, train_model, large_moneyness_penalty, butterfly_arbitrage_penalty, calendar_spread_penalty, safe_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IVDataset(df, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.get_input_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "dnn = IVSDNN(input_size=dataset.get_input_size(), hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_penalty=1\n",
    "num_epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ivs-dnn\")\n",
    "train_model(dnn, train_loader, 10, 0.001, 1, wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
