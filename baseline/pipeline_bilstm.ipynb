{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM for Feature Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bilstm import BiLSTMDatasetManager, BiLSTMModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/processed/features_pca_iv16-20.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BiLSTMDatasetManager(data_path)\n",
    "features, targets = dataset.make_train_target_pairs()\n",
    "print('Features shape:', features.shape)\n",
    "print('Targets shape:', targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing BiLSTM model...')\n",
    "model_path = './ckpts/test_bilstm256.pth'\n",
    "model = BiLSTMModelManager(input_dim=9, hidden_dim=256, output_dim=3, learning_rate=0.01, model_path=model_path)\n",
    "model.train(features, targets, epochs=250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from models.bilstm import CustomBiLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11350/4007657262.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bilstm_model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './ckpts/test_bilstm256.pth'\n",
    "bilstm_model = CustomBiLSTMModel(input_dim=9, hidden_dim=256, output_dim=3)\n",
    "bilstm_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>-2.946036</td>\n",
       "      <td>0.399305</td>\n",
       "      <td>2.333101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>-2.890993</td>\n",
       "      <td>0.153763</td>\n",
       "      <td>2.388737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>-4.476846</td>\n",
       "      <td>2.506460</td>\n",
       "      <td>2.701483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>-3.925725</td>\n",
       "      <td>1.810529</td>\n",
       "      <td>2.540009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>-4.277851</td>\n",
       "      <td>2.453446</td>\n",
       "      <td>2.562114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  feature1  feature2  feature3\n",
       "0  2016-01-05 -2.946036  0.399305  2.333101\n",
       "1  2016-01-06 -2.890993  0.153763  2.388737\n",
       "2  2016-01-07 -4.476846  2.506460  2.701483\n",
       "3  2016-01-08 -3.925725  1.810529  2.540009\n",
       "4  2016-01-11 -4.277851  2.453446  2.562114"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(\"../data/processed/features_pca_iv16-20.csv\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(22,len(features)):\n",
    "    ma1 = torch.tensor(features.iloc[i-1][['feature1', 'feature2', 'feature3']].astype(float).values, dtype=torch.float32)\n",
    "    ma5 = torch.tensor(features.iloc[i-5:i][['feature1', 'feature2', 'feature3']].mean(axis=0).values, dtype=torch.float32)\n",
    "    ma22 = torch.tensor(features.iloc[i-22:i][['feature1', 'feature2', 'feature3']].mean(axis=0).values, dtype=torch.float32)\n",
    "    feature = torch.cat((ma1, ma5, ma22), dim=0).to(device)\n",
    "    out = bilstm_model.predict(feature)\n",
    "    for obj in out:\n",
    "        features.at[i, \"F1\"] = obj[0].item()\n",
    "        features.at[i, \"F2\"] = obj[1].item()\n",
    "        features.at[i, \"F3\"] = obj[2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>-2.785850</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-08</td>\n",
       "      <td>-2.426245</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>2.072642</td>\n",
       "      <td>-2.711939</td>\n",
       "      <td>0.211495</td>\n",
       "      <td>2.214754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>-2.978782</td>\n",
       "      <td>0.438694</td>\n",
       "      <td>2.333295</td>\n",
       "      <td>-3.057659</td>\n",
       "      <td>0.473622</td>\n",
       "      <td>2.400317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>-3.273419</td>\n",
       "      <td>0.806418</td>\n",
       "      <td>2.418158</td>\n",
       "      <td>-3.468703</td>\n",
       "      <td>1.069636</td>\n",
       "      <td>2.478444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>-2.607269</td>\n",
       "      <td>0.123359</td>\n",
       "      <td>2.159899</td>\n",
       "      <td>-2.746451</td>\n",
       "      <td>0.177474</td>\n",
       "      <td>2.305250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  feature1  feature2  feature3        F1        F2        F3\n",
       "0  2016-02-05 -2.785850  0.208685  2.273442 -2.866897  0.122800  2.436066\n",
       "1  2016-02-08 -2.426245 -0.018148  2.072642 -2.711939  0.211495  2.214754\n",
       "2  2016-02-09 -2.978782  0.438694  2.333295 -3.057659  0.473622  2.400317\n",
       "3  2016-02-10 -3.273419  0.806418  2.418158 -3.468703  1.069636  2.478444\n",
       "4  2016-02-11 -2.607269  0.123359  2.159899 -2.746451  0.177474  2.305250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features.dropna().reset_index(drop=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv_path_list = [\n",
    "    \"../data/processed/pca/predicted_iv16.csv\",\n",
    "    \"../data/processed/pca/predicted_iv17.csv\",\n",
    "    \"../data/processed/pca/predicted_iv18.csv\",\n",
    "    \"../data/processed/pca/predicted_iv19.csv\",\n",
    "    \"../data/processed/pca/predicted_iv20.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tau</th>\n",
       "      <th>m</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>0.326153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.291228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.105361</td>\n",
       "      <td>0.286565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.051293</td>\n",
       "      <td>0.286299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.286591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       tau         m        IV\n",
       "0  2016-01-04  0.027397 -0.510826  0.326153\n",
       "1  2016-01-04  0.027397 -0.223144  0.291228\n",
       "2  2016-01-04  0.027397 -0.105361  0.286565\n",
       "3  2016-01-04  0.027397 -0.051293  0.286299\n",
       "4  2016-01-04  0.027397 -0.025318  0.286591"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for path in df_iv_path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    merged_df = pd.concat([merged_df, df], axis=0)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "print(len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tau</th>\n",
       "      <th>m</th>\n",
       "      <th>IV</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>0.346719</td>\n",
       "      <td>-2.78585</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.309534</td>\n",
       "      <td>-2.78585</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.105361</td>\n",
       "      <td>0.304773</td>\n",
       "      <td>-2.78585</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.051293</td>\n",
       "      <td>0.304623</td>\n",
       "      <td>-2.78585</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.305006</td>\n",
       "      <td>-2.78585</td>\n",
       "      <td>0.208685</td>\n",
       "      <td>2.273442</td>\n",
       "      <td>-2.866897</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>2.436066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       tau         m        IV  feature1  feature2  feature3  \\\n",
       "0  2016-02-05  0.027397 -0.510826  0.346719  -2.78585  0.208685  2.273442   \n",
       "1  2016-02-05  0.027397 -0.223144  0.309534  -2.78585  0.208685  2.273442   \n",
       "2  2016-02-05  0.027397 -0.105361  0.304773  -2.78585  0.208685  2.273442   \n",
       "3  2016-02-05  0.027397 -0.051293  0.304623  -2.78585  0.208685  2.273442   \n",
       "4  2016-02-05  0.027397 -0.025318  0.305006  -2.78585  0.208685  2.273442   \n",
       "\n",
       "         F1      F2        F3  \n",
       "0 -2.866897  0.1228  2.436066  \n",
       "1 -2.866897  0.1228  2.436066  \n",
       "2 -2.866897  0.1228  2.436066  \n",
       "3 -2.866897  0.1228  2.436066  \n",
       "4 -2.866897  0.1228  2.436066  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the two dataframes using the date column so that we have the corresponding F1, F2, F3 values for each date\n",
    "df = pd.merge(merged_df, features, on='date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['F1', 'F2', 'F3']\n",
    "from models.dnn import IVDataset, IVSDNN, train_model, large_moneyness_penalty, butterfly_arbitrage_penalty, calendar_spread_penalty, safe_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor shapes:\n",
      "Features: torch.Size([189882, 3])\n",
      "m: torch.Size([189882, 1])\n",
      "tau: torch.Size([189882, 1])\n",
      "iv: torch.Size([189882, 1])\n",
      "\n",
      "Checking for NaN values:\n",
      "Features NaN: False\n",
      "m NaN: False\n",
      "tau NaN: False\n",
      "iv NaN: False\n"
     ]
    }
   ],
   "source": [
    "dataset = IVDataset(df, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(dataset.get_input_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "dnn = IVSDNN(input_size=dataset.get_input_size(), hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_penalty=1\n",
    "num_epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhigyanshanker\u001b[0m (\u001b[33mabx-group\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shankerabhigyan/code/thesis/thesis-IVS/baseline/wandb/run-20241112_163428-la156b0a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abx-group/ivs-dnn/runs/la156b0a' target=\"_blank\">amber-cherry-14</a></strong> to <a href='https://wandb.ai/abx-group/ivs-dnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abx-group/ivs-dnn' target=\"_blank\">https://wandb.ai/abx-group/ivs-dnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abx-group/ivs-dnn/runs/la156b0a' target=\"_blank\">https://wandb.ai/abx-group/ivs-dnn/runs/la156b0a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shankerabhigyan/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 || Loss = 11.582334 || Penalty = 11.503100 || Calendar Penalty = 14.281682 || Butterfly Penalty = 12.526200 || Large Moneyness Penalty = 30.707616\n",
      "Epoch 2 || Loss = 0.057450 || Penalty = 0.000158 || Calendar Penalty = 0.000384 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000011\n",
      "Epoch 3 || Loss = 0.057395 || Penalty = 0.000103 || Calendar Penalty = 0.000170 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000002\n",
      "Epoch 4 || Loss = 0.057370 || Penalty = 0.000077 || Calendar Penalty = 0.000096 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000001\n",
      "Epoch 5 || Loss = 0.057353 || Penalty = 0.000060 || Calendar Penalty = 0.000060 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 6 || Loss = 0.057338 || Penalty = 0.000044 || Calendar Penalty = 0.000044 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 7 || Loss = 0.057331 || Penalty = 0.000038 || Calendar Penalty = 0.000038 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 8 || Loss = 0.057328 || Penalty = 0.000035 || Calendar Penalty = 0.000035 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 9 || Loss = 0.057327 || Penalty = 0.000034 || Calendar Penalty = 0.000033 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 10 || Loss = 0.057325 || Penalty = 0.000033 || Calendar Penalty = 0.000033 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 11 || Loss = 0.057325 || Penalty = 0.000033 || Calendar Penalty = 0.000033 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 12 || Loss = 0.057327 || Penalty = 0.000033 || Calendar Penalty = 0.000033 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 13 || Loss = 0.057327 || Penalty = 0.000034 || Calendar Penalty = 0.000034 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 14 || Loss = 0.057325 || Penalty = 0.000034 || Calendar Penalty = 0.000034 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 15 || Loss = 0.057326 || Penalty = 0.000034 || Calendar Penalty = 0.000034 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 16 || Loss = 0.057327 || Penalty = 0.000034 || Calendar Penalty = 0.000034 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 17 || Loss = 0.057326 || Penalty = 0.000034 || Calendar Penalty = 0.000034 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 18 || Loss = 0.057326 || Penalty = 0.000033 || Calendar Penalty = 0.000033 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 19 || Loss = 0.057325 || Penalty = 0.000032 || Calendar Penalty = 0.000032 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 20 || Loss = 0.057325 || Penalty = 0.000032 || Calendar Penalty = 0.000032 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 21 || Loss = 0.057324 || Penalty = 0.000032 || Calendar Penalty = 0.000032 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 22 || Loss = 0.057325 || Penalty = 0.000031 || Calendar Penalty = 0.000031 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 23 || Loss = 0.057324 || Penalty = 0.000031 || Calendar Penalty = 0.000031 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 24 || Loss = 0.057322 || Penalty = 0.000030 || Calendar Penalty = 0.000030 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 25 || Loss = 0.057322 || Penalty = 0.000029 || Calendar Penalty = 0.000029 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 26 || Loss = 0.057321 || Penalty = 0.000028 || Calendar Penalty = 0.000028 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 27 || Loss = 0.057321 || Penalty = 0.000027 || Calendar Penalty = 0.000027 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 28 || Loss = 0.057318 || Penalty = 0.000026 || Calendar Penalty = 0.000026 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 29 || Loss = 0.057317 || Penalty = 0.000025 || Calendar Penalty = 0.000025 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 30 || Loss = 0.057317 || Penalty = 0.000024 || Calendar Penalty = 0.000024 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 31 || Loss = 0.057316 || Penalty = 0.000023 || Calendar Penalty = 0.000023 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 32 || Loss = 0.057314 || Penalty = 0.000022 || Calendar Penalty = 0.000022 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 33 || Loss = 0.057313 || Penalty = 0.000021 || Calendar Penalty = 0.000021 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 34 || Loss = 0.057314 || Penalty = 0.000020 || Calendar Penalty = 0.000020 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 35 || Loss = 0.057313 || Penalty = 0.000019 || Calendar Penalty = 0.000019 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 36 || Loss = 0.057311 || Penalty = 0.000019 || Calendar Penalty = 0.000019 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 37 || Loss = 0.057310 || Penalty = 0.000018 || Calendar Penalty = 0.000018 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 38 || Loss = 0.057310 || Penalty = 0.000017 || Calendar Penalty = 0.000017 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 39 || Loss = 0.057310 || Penalty = 0.000017 || Calendar Penalty = 0.000017 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 40 || Loss = 0.057308 || Penalty = 0.000016 || Calendar Penalty = 0.000016 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 41 || Loss = 0.057308 || Penalty = 0.000016 || Calendar Penalty = 0.000016 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 42 || Loss = 0.057307 || Penalty = 0.000015 || Calendar Penalty = 0.000015 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 43 || Loss = 0.057308 || Penalty = 0.000015 || Calendar Penalty = 0.000015 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 44 || Loss = 0.057307 || Penalty = 0.000015 || Calendar Penalty = 0.000015 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 45 || Loss = 0.057308 || Penalty = 0.000015 || Calendar Penalty = 0.000015 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 46 || Loss = 0.057308 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 47 || Loss = 0.057307 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 48 || Loss = 0.057306 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 49 || Loss = 0.057306 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 50 || Loss = 0.057307 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 51 || Loss = 0.057306 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 52 || Loss = 0.057306 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 53 || Loss = 0.057308 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 54 || Loss = 0.057307 || Penalty = 0.000014 || Calendar Penalty = 0.000014 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 55 || Loss = 0.057304 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 56 || Loss = 0.057305 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 57 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 58 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 59 || Loss = 0.057307 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 60 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 61 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 62 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 63 || Loss = 0.057304 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 64 || Loss = 0.057306 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n",
      "Epoch 65 || Loss = 0.057307 || Penalty = 0.000013 || Calendar Penalty = 0.000013 || Butterfly Penalty = 0.000000 || Large Moneyness Penalty = 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mivs-dnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/thesis/thesis-IVS/baseline/models/dnn.py:167\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, lambda_penalty, wandb)\u001b[0m\n\u001b[1;32m    164\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse \u001b[38;5;241m+\u001b[39m penalty\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Backward pass with gradient clipping\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    169\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ivs-dnn\")\n",
    "train_model(dnn, train_loader, 200, 0.001, 1, wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
